{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/hannahzhang/Desktop/Github Repos/ERSP-TeamYang/data/bm25-t5-dev.trec', '/Users/hannahzhang/Desktop/Github Repos/ERSP-TeamYang/data/splade-dev.trec']\n",
      "['/Users/hannahzhang/Desktop/Github Repos/ERSP-TeamYang/data/qrels.train.tsv', '/Users/hannahzhang/Desktop/Github Repos/ERSP-TeamYang/data/qrels.dev.tsv']\n",
      "['/Users/hannahzhang/Desktop/Github Repos/ERSP-TeamYang/data/queries/queries.eval.tsv', '/Users/hannahzhang/Desktop/Github Repos/ERSP-TeamYang/data/queries/queries.train.tsv', '/Users/hannahzhang/Desktop/Github Repos/ERSP-TeamYang/data/queries/queries.dev.tsv']\n",
      "/Users/hannahzhang/Desktop/Github Repos/ERSP-TeamYang/data/collection.tsv\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/hannahzhang/Desktop/Github Repos/ERSP-TeamYang/data/\"\n",
    "\n",
    "models = []\n",
    "qrels = []\n",
    "queries = []\n",
    "collection = data_dir + 'collection.tsv' # collection file\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.trec'): # trec files\n",
    "        models.append(data_dir + filename)\n",
    "    elif filename.split('.')[0] == \"qrels\": # qrels files\n",
    "        qrels.append(data_dir + filename) # queries folder\n",
    "    elif filename == 'queries': \n",
    "        queries_dir = data_dir + filename + \"/\"\n",
    "        for filename in os.listdir(queries_dir):\n",
    "            queries.append(queries_dir + filename)\n",
    "\n",
    "print(models)\n",
    "print(qrels)\n",
    "print(queries)\n",
    "print(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1185869  0      0.1  1\n",
      "0       1185868  0       16  1\n",
      "1        597651  0       49  1\n",
      "2        403613  0       60  1\n",
      "3       1183785  0      389  1\n",
      "4        312651  0      616  1\n",
      "...         ... ..      ... ..\n",
      "532755    19285  0  8841362  1\n",
      "532756   558837  0  4989159  1\n",
      "532757   559149  0  8841547  1\n",
      "532758   706678  0  8841643  1\n",
      "532759   405466  0  8841735  1\n",
      "\n",
      "[532760 rows x 4 columns]\n",
      "       1102432  0  2026790  1\n",
      "0      1102431  0  7066866  1\n",
      "1      1102431  0  7066867  1\n",
      "2      1090282  0  7066900  1\n",
      "3        39449  0  7066905  1\n",
      "4        76162  0  7066915  1\n",
      "...        ... ..      ... ..\n",
      "59267   150337  0  8009410  1\n",
      "59268    22241  0  8009429  1\n",
      "59269   129177  0  8009442  1\n",
      "59270   190655  0  3576091  1\n",
      "59271   371455  0  8009476  1\n",
      "\n",
      "[59272 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "columns = []\n",
    "\n",
    "qrels_train_df = pd.read_csv(qrels[0], sep=\"\\t\")\n",
    "print(qrels_train_df)\n",
    "\n",
    "qrels_dev_df = pd.read_csv(qrels[1], sep=\"\\t\")\n",
    "print(qrels_dev_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Descriptions\n",
    "- Query_id: Id of query that system is being evaluated on\n",
    "- Iteration: Iteration number (or run) of the retrieval system\n",
    "  - Q0 means its the first (or only) run (e.g. batch of results) for the query\n",
    "- Document_id: Id of document (or passage) returned by the retrieval system for the query\n",
    "- Rank: Rank of the document for the query\n",
    "- Score: Relevance score assigned by the retrieval system to the document for the query\n",
    "- Run_id: Unique identifier for the run that produced the result (often used to distinguish between different models or different configurations of the same model in the testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         query_id iteration  document_id  rank    score run_id\n",
      "0         1048585        Q0      7187157     0  46.6189     R0\n",
      "1         1048585        Q0      7187156     1  45.7317     R0\n",
      "2         1048585        Q0      7187158     2  44.6009     R0\n",
      "3         1048585        Q0      7617404     3  44.5052     R0\n",
      "4         1048585        Q0      7187155     4  43.8783     R0\n",
      "...           ...       ...          ...   ...      ...    ...\n",
      "6979995   1048565        Q0      3922376   995  17.2643     R0\n",
      "6979996   1048565        Q0       765915   996  17.2637     R0\n",
      "6979997   1048565        Q0      4292320   997  17.2631     R0\n",
      "6979998   1048565        Q0      1695524   998  17.2628     R0\n",
      "6979999   1048565        Q0      2985688   999  17.2608     R0\n",
      "\n",
      "[6980000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "columns = ['query_id', 'iteration', 'document_id', 'rank', 'score', 'run_id']\n",
    "\n",
    "bm25_df = pd.read_csv(models[0], sep='\\t', names=columns)\n",
    "\n",
    "print(bm25_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique queries:  [1048585       2  524332 ...  968921  786375 1048565]\n",
      "Unique query count:  6980\n"
     ]
    }
   ],
   "source": [
    "unique_queries = bm25_df['query_id'].unique()\n",
    "unique_query_count = bm25_df['query_id'].nunique()\n",
    "\n",
    "print(\"Unique queries: \", unique_queries)\n",
    "print(\"Unique query count: \", unique_query_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique document count for Query ID 1048585:  1000 \n",
      "5 Unique documents for Query ID 1048585:  [7187157 7187156 7187158 7617404 7187155]\n",
      "Unique document count for Query ID 2:  1000 \n",
      "5 Unique documents for Query ID 2:  [5262269 1304571 5881917 3857958 6947077]\n",
      "Unique document count for Query ID 524332:  1000 \n",
      "5 Unique documents for Query ID 524332:  [1518543 1512632 1966060 1194314 1518541]\n",
      "Unique document count for Query ID 1048642:  1000 \n",
      "5 Unique documents for Query ID 1048642:  [ 671694 8621225  906008 8041183 4734246]\n",
      "Unique document count for Query ID 524447:  1000 \n",
      "5 Unique documents for Query ID 524447:  [3541560 8454456 3541558 3836580 6856971]\n"
     ]
    }
   ],
   "source": [
    "for i, n in enumerate(unique_queries[:5]):\n",
    "    uq_df = bm25_df[bm25_df['query_id'] == unique_queries[i]] # Splice df on unique query\n",
    "    unique_documents = uq_df['document_id'].unique()\n",
    "    unique_document_count = uq_df['document_id'].nunique()\n",
    "    print(\n",
    "        f\"Unique document count for Query ID {n}: \", unique_document_count,\n",
    "        f\"\\n5 Unique documents for Query ID {n}: \", unique_documents[:5]\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         query_id iteration  document_id  rank   score run_id\n",
      "0         1048585        Q0      7187155     0  104472     R0\n",
      "1         1048585        Q0      7187160     1  100811     R0\n",
      "2         1048585        Q0      7187157     2   99206     R0\n",
      "3         1048585        Q0      7187158     3   98698     R0\n",
      "4         1048585        Q0      3100835     4   86255     R0\n",
      "...           ...       ...          ...   ...     ...    ...\n",
      "6979995   1048565        Q0      4838288   995   66246     R0\n",
      "6979996   1048565        Q0      2133477   996   66245     R0\n",
      "6979997   1048565        Q0      5753707   997   66239     R0\n",
      "6979998   1048565        Q0      1472257   998   66238     R0\n",
      "6979999   1048565        Q0      5637117   999   66238     R0\n",
      "\n",
      "[6980000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "columns = ['query_id', 'iteration', 'document_id', 'rank', 'score', 'run_id']\n",
    "\n",
    "splade_df = pd.read_csv(models[1], sep='\\t', names=columns)\n",
    "\n",
    "print(splade_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique query count:  6980\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique query count: \", bm25_df['query_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df = pd.read_csv(collection, sep='\\t', names=['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The presence of communication amid scientific ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Manhattan Project and its atomic bomb help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Essay on The Manhattan Project - The Manhattan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Manhattan Project was the name for a proje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>versions of each volume as well as complementa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query\n",
       "0  The presence of communication amid scientific ...\n",
       "1  The Manhattan Project and its atomic bomb help...\n",
       "2  Essay on The Manhattan Project - The Manhattan...\n",
       "3  The Manhattan Project was the name for a proje...\n",
       "4  versions of each volume as well as complementa..."
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8841823, 1)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.\n",
      "[('the Manhattan Project', 'ORG'), ('hundreds of thousands', 'CARDINAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy pretrained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "named_entities = []\n",
    "\n",
    "# Loop over the first 10 rows\n",
    "for i in range(500):\n",
    "    # Get the text from the 'query' column of the dataset\n",
    "    text = collection_df['query'][i]\n",
    "\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract named entities from the text\n",
    "    ents = [(ent.text, ent.label_) for ent in doc.ents]  # List of (entity, label) tuples\n",
    "    \n",
    "    # Append the query and its corresponding entities as a tuple\n",
    "    named_entities.append({'query': text, 'entities': ents})\n",
    "\n",
    "# Convert the list into a DataFrame\n",
    "entities_df = pd.DataFrame(named_entities)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(entities_df.iloc[0, 0])\n",
    "print(entities_df.iloc[0, 1])\n",
    "\n",
    "## If contains 1 entity cateogory, may be useful for sparse learning as hold more distinguishable information. \n",
    "## If contains multiple, might be better to use sparse learning.\n",
    "\n",
    "## binary easier, use weights for normalize frequency of query category, PCA feature extraction first\n",
    "\n",
    "## mrr to go through query categories (to determine whether the classification is accurate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 44), ('first', 40), ('two', 32), ('2', 27), ('one', 25), ('China', 22), ('the United States', 21), ('Latrobe', 20), ('annual', 18), ('Scottish', 17)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of entities and count their occurrences\n",
    "all_entities = [ent[0] for sublist in entities_df['entities'] for ent in sublist]\n",
    "entity_counts = Counter(all_entities)\n",
    "\n",
    "# Print the most common entities\n",
    "print(entity_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n"
     ]
    }
   ],
   "source": [
    "# Filter queries that mention person\n",
    "person_queries = entities_df[entities_df['entities'].apply(lambda x: any(ent[1] == 'PERSON' for ent in x))]\n",
    "print(len(person_queries))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
