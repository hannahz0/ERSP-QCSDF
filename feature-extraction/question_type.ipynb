{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from tensorflow.keras import models, layers, optimizers, losses, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/hannahzhang/Desktop/Github Repos/ERSP-TeamYang/data/\"\n",
    "\n",
    "dataset = data_dir + \"Question_Classification_Dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       Unnamed: 0                                          Questions  \\\n",
      "0              0  How did serfdom develop in and then leave Russ...   \n",
      "1              1   What films featured the character Popeye Doyle ?   \n",
      "2              2  How can I find a list of celebrities ' real na...   \n",
      "3              3  What fowl grabs the spotlight after the Chines...   \n",
      "4              4                    What is the full form of .com ?   \n",
      "...          ...                                                ...   \n",
      "5447        5447            What 's the shape of a camel 's spine ?   \n",
      "5448        5448           What type of currency is used in China ?   \n",
      "5449        5449                    What is the temperature today ?   \n",
      "5450        5450              What is the temperature for cooking ?   \n",
      "5451        5451               What currency is used in Australia ?   \n",
      "\n",
      "         Category0 Category1 Category2  \n",
      "0      DESCRIPTION      DESC    manner  \n",
      "1           ENTITY      ENTY    cremat  \n",
      "2      DESCRIPTION      DESC    manner  \n",
      "3           ENTITY      ENTY    animal  \n",
      "4     ABBREVIATION      ABBR       exp  \n",
      "...            ...       ...       ...  \n",
      "5447        ENTITY      ENTY     other  \n",
      "5448        ENTITY      ENTY  currency  \n",
      "5449       NUMERIC       NUM      temp  \n",
      "5450       NUMERIC       NUM      temp  \n",
      "5451        ENTITY      ENTY  currency  \n",
      "\n",
      "[5452 rows x 5 columns]>\n",
      "Index(['Unnamed: 0', 'Questions', 'Category0', 'Category1', 'Category2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(dataset)\n",
    "print(df.head)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                               Questions     Category0\n",
       "0     How did serfdom develop in and then leave Russ...   DESCRIPTION\n",
       "1      What films featured the character Popeye Doyle ?        ENTITY\n",
       "2     How can I find a list of celebrities ' real na...   DESCRIPTION\n",
       "3     What fowl grabs the spotlight after the Chines...        ENTITY\n",
       "4                       What is the full form of .com ?  ABBREVIATION\n",
       "...                                                 ...           ...\n",
       "5447            What 's the shape of a camel 's spine ?        ENTITY\n",
       "5448           What type of currency is used in China ?        ENTITY\n",
       "5449                    What is the temperature today ?       NUMERIC\n",
       "5450              What is the temperature for cooking ?       NUMERIC\n",
       "5451               What currency is used in Australia ?        ENTITY\n",
       "\n",
       "[5452 rows x 2 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns = ['Unnamed: 0', 'Category1', 'Category2'])\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ABBREVIATION  DESCRIPTION  ENTITY  HUMAN  LOCATION  NUMERIC\n",
      "0            False         True   False  False     False    False\n",
      "1            False        False    True  False     False    False\n",
      "2            False         True   False  False     False    False\n",
      "3            False        False    True  False     False    False\n",
      "4             True        False   False  False     False    False\n",
      "...            ...          ...     ...    ...       ...      ...\n",
      "5447         False        False    True  False     False    False\n",
      "5448         False        False    True  False     False    False\n",
      "5449         False        False   False  False     False     True\n",
      "5450         False        False   False  False     False     True\n",
      "5451         False        False    True  False     False    False\n",
      "\n",
      "[5452 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "y = pd.get_dummies(df['Category0'])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABBREVIATION', 'DESCRIPTION', 'ENTITY', 'HUMAN', 'LOCATION', 'NUMERIC']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = list(y.columns)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove html tags\n",
    "def removeHTML(sentence):\n",
    "    regex = re.compile('<.*?>')\n",
    "    return re.sub(regex, ' ', sentence)\n",
    "\n",
    "# Remove URLs\n",
    "def removeURL(sentence):\n",
    "    regex = re.compile('http[s]?://\\S+')\n",
    "    return re.sub(regex, ' ', sentence)\n",
    "\n",
    "# remove numbers, punctuation and any special characters (keep only alphabets)\n",
    "def onlyAlphabets(sentence):\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    return re.sub(regex, ' ', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sno = nltk.stem.SnowballStemmer('english')    # Initializing stemmer\n",
    "wordcloud = [[], [], [], [], [], [], []]\n",
    "all_sentences = []    # All cleaned sentences\n",
    "\n",
    "\n",
    "for x in range(len(df['Questions'].values)):\n",
    "    question = df['Questions'].values[x]\n",
    "    classname = df['Category0'].values[x]\n",
    "\n",
    "    cleaned_sentence = []\n",
    "    sentence = removeURL(question) \n",
    "    sentence = removeHTML(sentence)\n",
    "    sentence = onlyAlphabets(sentence)\n",
    "    sentence = sentence.lower()   \n",
    "\n",
    "    for word in sentence.split():\n",
    "        #if word not in stop:\n",
    "            stemmed = sno.stem(word)\n",
    "            cleaned_sentence.append(stemmed)\n",
    "            \n",
    "            wordcloud[class_names.index(classname)].append(word)\n",
    "            \n",
    "\n",
    "    all_sentences.append(' '.join(cleaned_sentence))\n",
    "\n",
    "# add as column in dataframe\n",
    "X = all_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = 1500\n",
    "mlen = 200\n",
    " \n",
    "tokenizer = Tokenizer(num_words = vocab, oov_token = '<UNK>')\n",
    "tokenizer.fit_on_texts(Xtrain)\n",
    " \n",
    "Xtrain = tokenizer.texts_to_sequences(Xtrain)\n",
    "Xtrain = pad_sequences(Xtrain, maxlen=mlen)\n",
    "\n",
    "Xval = tokenizer.texts_to_sequences(Xval)\n",
    "Xval = pad_sequences(Xval, maxlen=mlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannahzhang/.pyenv/versions/3.10.13/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 167ms/step - accuracy: 0.3773 - loss: 1.4590 - val_accuracy: 0.7754 - val_loss: 0.7339\n",
      "Epoch 2/256\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.8446 - loss: 0.5135 - val_accuracy: 0.8213 - val_loss: 0.5634\n",
      "Epoch 3/256\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.9069 - loss: 0.3184 - val_accuracy: 0.8249 - val_loss: 0.6148\n",
      "Epoch 4/256\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 169ms/step - accuracy: 0.9397 - loss: 0.2086 - val_accuracy: 0.8579 - val_loss: 0.5665\n",
      "Epoch 5/256\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 169ms/step - accuracy: 0.9565 - loss: 0.1570 - val_accuracy: 0.8570 - val_loss: 0.5364\n",
      "Epoch 6/256\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.9653 - loss: 0.1214 - val_accuracy: 0.8478 - val_loss: 0.6215\n",
      "Epoch 7/256\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.9747 - loss: 0.0820 - val_accuracy: 0.8561 - val_loss: 0.6851\n",
      "Epoch 8/256\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.9709 - loss: 0.0952 - val_accuracy: 0.8442 - val_loss: 0.6700\n",
      "Epoch 9/256\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 168ms/step - accuracy: 0.9793 - loss: 0.0650 - val_accuracy: 0.8451 - val_loss: 0.6273\n",
      "Epoch 10/256\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 169ms/step - accuracy: 0.9826 - loss: 0.0560 - val_accuracy: 0.8643 - val_loss: 0.6601\n"
     ]
    }
   ],
   "source": [
    "# Build and train neural network\n",
    "embedding_dim = 128\n",
    " \n",
    "model = models.Sequential([\n",
    "    layers.Embedding(vocab, embedding_dim, input_length = mlen),\n",
    "    layers.LSTM(128, activation='tanh'),\n",
    "    layers.Dense(64, activation = 'relu'),\n",
    "    layers.Dense(32, activation = 'relu'),\n",
    "    layers.Dense(len(class_names), activation = 'softmax')\n",
    "])\n",
    " \n",
    "cb = [callbacks.EarlyStopping(patience = 5, restore_best_weights = True)]\n",
    "\n",
    "model.compile(optimizer = optimizers.Adam(0.01), loss = losses.CategoricalCrossentropy(), metrics = ['accuracy'])\n",
    "history = model.fit(Xtrain, ytrain, batch_size=64, epochs = 256, validation_data=(Xval, yval), callbacks = cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8619 - loss: 0.5147\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "F1 score:  0.8568663544140677\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(Xval, yval)\n",
    "\n",
    "print(\"F1 score: \", f1_score(np.argmax(yval.to_numpy(), axis = 1), np.argmax(model.predict(Xval), axis = 1), average = 'weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
