{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Weights\n",
    "\n",
    "- Goal: Manual Search\n",
    "    1. Initialize range for alpha (0 to 1)\n",
    "    2. Combine sparse and dense scores for each combination of alpha (dense) and 1 - alpha (sparse)\n",
    "    3. Use evaluation metric (MRR) to find best combination of weights\n",
    "\n",
    "- Evaluation metric: NDCG (Normalized Discounted Cumulative Gain)\n",
    "    - Prioritizes top-ranked documents by penalizing rankings that place relevant documents lower down in the ranking list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "- for both splade and bm25, since qrels only provides us with 1 confirmed relevant document, should we use mrr to as an evaluation metric since it only looks at the first relevant document?\n",
    "    - use mrr\n",
    "\n",
    "- how to handle weighting when one model ranks a document that the other does not rank at all? \n",
    "    - the second model gives it a zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ndcg_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/hannahzhang/Desktop/Github Repos/ERSP-TeamYang/data/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".tsv\") or file.endswith(\".trec\"):\n",
    "        data.append(data_dir + file)\n",
    "\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splade dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          Query ID  Document ID  Rank   Score\n",
      "0         1048585      7187155     0  104472\n",
      "1         1048585      7187160     1  100811\n",
      "2         1048585      7187157     2   99206\n",
      "3         1048585      7187158     3   98698\n",
      "4         1048585      3100835     4   86255\n",
      "...           ...          ...   ...     ...\n",
      "6979995   1048565      4838288   995   66246\n",
      "6979996   1048565      2133477   996   66245\n",
      "6979997   1048565      5753707   997   66239\n",
      "6979998   1048565      1472257   998   66238\n",
      "6979999   1048565      5637117   999   66238\n",
      "\n",
      "[6980000 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "splade_df = pd.read_csv(data[3], sep=\"\\t\", names=['Query ID', 'Q0', 'Document ID', 'Rank', 'Score', 'R0'])\n",
    "splade_df = splade_df.drop(splade_df.columns[[1,5]], axis=1)\n",
    "print(splade_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          Query ID  Document ID     Score\n",
      "0         1048585      7187157  0.866932\n",
      "1         1048585      7187158  0.863535\n",
      "2         1048585      7187155  0.861530\n",
      "3         1048585      7187160  0.858853\n",
      "4         1048585      7187163  0.840336\n",
      "...           ...          ...       ...\n",
      "6979995   1048565      4529995  0.705006\n",
      "6979996   1048565      8496497  0.704949\n",
      "6979997   1048565      5713758  0.699297\n",
      "6979998   1048565      1778769  0.695161\n",
      "6979999   1048565      5713765  0.689829\n",
      "\n",
      "[6980000 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "dense_df = pd.read_csv(data[1], sep=\"\\t\", names=['Query ID', 'Document ID', 'Score'])\n",
    "print(dense_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        Query ID  Document ID  Relevance\n",
       "0       1102432      2026790          1\n",
       "1       1102431      7066866          1\n",
       "2       1102431      7066867          1\n",
       "3       1090282      7066900          1\n",
       "4         39449      7066905          1\n",
       "...         ...          ...        ...\n",
       "59268    150337      8009410          1\n",
       "59269     22241      8009429          1\n",
       "59270    129177      8009442          1\n",
       "59271    190655      3576091          1\n",
       "59272    371455      8009476          1\n",
       "\n",
       "[59273 rows x 3 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels_df = pd.read_csv(data[2], sep=\"\\t\", names=['Query ID', '0', 'Document ID', \"Relevance\"])\n",
    "qrels_df = qrels_df.drop(columns=['0'])\n",
    "\n",
    "qrels_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6980\n"
     ]
    }
   ],
   "source": [
    "dense_query_ids = []\n",
    "sparse_query_ids = []\n",
    "\n",
    "\n",
    "for x in dense_df['Query ID'].unique():\n",
    "    dense_query_ids.append(int(x))\n",
    "\n",
    "for x in splade_df['Query ID'].unique():\n",
    "    sparse_query_ids.append(int(x))\n",
    "\n",
    "query_ids = list(set(dense_query_ids) & set(sparse_query_ids))\n",
    "\n",
    "print(len(query_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13\n",
      " 0.14 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27\n",
      " 0.28 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41\n",
      " 0.42 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55\n",
      " 0.56 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69\n",
      " 0.7  0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83\n",
      " 0.84 0.85 0.86 0.87 0.88 0.89 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97\n",
      " 0.98 0.99 1.  ]\n"
     ]
    }
   ],
   "source": [
    "alpha_values = np.arange(0, 1.01, 0.01)\n",
    "print(alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_weights(dense_df, sparse_df, query_id, alpha_values, qrels_df):\n",
    "    best_alpha = 0\n",
    "    best_mrr = 0\n",
    "\n",
    "    for alpha in alpha_values:\n",
    "        # Filter by query ID\n",
    "        filtered_dense_df = dense_df[dense_df[\"Query ID\"] == query_id].copy()\n",
    "        filtered_sparse_df = sparse_df[sparse_df[\"Query ID\"] == query_id].copy()\n",
    "\n",
    "        # Find weighted scores\n",
    "        filtered_dense_df[\"Score\"] *= (1 - alpha)\n",
    "        filtered_sparse_df[\"Score\"] *= alpha\n",
    "\n",
    "        # Merge rankings\n",
    "        merged = filtered_dense_df.merge(filtered_sparse_df, on=\"Document ID\", how=\"outer\", suffixes=(\"_dense\", \"_sparse\")).fillna(0)\n",
    "        merged[\"Final Score\"] = merged[\"Score_dense\"] + merged[\"Score_sparse\"]\n",
    "\n",
    "        # Rank documents\n",
    "        ranked_results = merged.sort_values(\"Final Score\", ascending=False)\n",
    "        ranked_docs = ranked_results[\"Document ID\"].tolist()\n",
    "\n",
    "        # MRR\n",
    "        relevant_doc = qrels_df[qrels_df[\"Query ID\"] == query_id][\"Document ID\"].iloc[0]\n",
    "        if relevant_doc not in ranked_docs:\n",
    "            mrr_score = 0\n",
    "        else:\n",
    "            rank = ranked_docs.index(relevant_doc) + 1\n",
    "            mrr_score = 1 / rank\n",
    "\n",
    "        # Update alpha and MRR\n",
    "        if mrr_score > best_mrr:\n",
    "            best_mrr = mrr_score\n",
    "            best_alpha = alpha\n",
    "\n",
    "    return best_alpha, best_mrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alphas = []\n",
    "best_mrrs = []\n",
    "\n",
    "for query in query_ids:\n",
    "    best_alpha, best_mrr = best_weights(dense_df, splade_df, query, alpha_values, qrels_df)\n",
    "    # print(f\"Query {query}: Best alpha: {best_alpha}, Best MRR: {best_mrr}\")\n",
    "    best_alphas.append(best_alpha)\n",
    "    best_mrrs.append(best_mrr)\n",
    "\n",
    "best_weights = {'Query ID': query_ids, 'Best Alpha': best_alphas, 'MRR': best_mrrs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_best_alphas = list(dict.fromkeys(best_alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.01\n",
      "0.02\n",
      "0.05\n",
      "0.04\n",
      "1.0\n",
      "0.09\n",
      "0.03\n",
      "0.22\n",
      "0.15\n",
      "0.1\n",
      "0.07\n",
      "0.08\n",
      "0.06\n",
      "0.3\n",
      "0.11\n",
      "0.13\n",
      "0.17\n",
      "0.23\n",
      "0.47000000000000003\n",
      "0.14\n",
      "0.16\n",
      "0.29\n",
      "0.12\n",
      "0.18\n"
     ]
    }
   ],
   "source": [
    "for i in unique_best_alphas:\n",
    "    print(float(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"best_weights.csv\", \"w\") as outfile:\n",
    "\twriter = csv.writer(outfile)\n",
    "\t\n",
    "\t# convert dict keys to a list\n",
    "\tkey_list = list(best_weights.keys())\n",
    "\t\n",
    "\twriter.writerow(best_weights.keys())\n",
    "\t\n",
    "\t# iterate each column and assign corresponding values to each column\n",
    "\tfor i in range(6980):\n",
    "\t\twriter.writerow([best_weights[x][i] for x in key_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Query ID  Best Alpha   MRR\n",
       "0            2        0.00  1.00\n",
       "1      1048585        0.00  0.50\n",
       "2       458771        0.00  1.00\n",
       "3       163860        0.01  0.25\n",
       "4       458774        0.00  0.50\n",
       "...        ...         ...   ...\n",
       "6975    884722        0.00  1.00\n",
       "6976    393203        0.00  0.20\n",
       "6977    196596        0.01  0.50\n",
       "6978   1048565        0.01  0.20\n",
       "6979   1081338        0.00  1.00\n",
       "\n",
       "[6980 rows x 3 columns]>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_weights_df = pd.read_csv(\"best_weights.csv\")\n",
    "\n",
    "best_weights_df.head\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
